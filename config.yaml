hyperparameters:
  model_list:
    # - mobilenetv1_100
    # - mobilenetv2_050
    # - mobilenetv2_100
    # - mobilenetv2_110d
    # - mobilenetv2_120d
    - mobilenetv2_140
    - mobilenetv3_small_050
    - mobilenetv3_small_075
    - mobilenetv3_small_100
    - mobilenetv3_large_100
    - mobilenetv3_large_150d
    - mobilenetv3_rw
    - mobilenetv4_conv_small_050
    - mobilenetv4_conv_small
    - mobilenetv4_conv_medium
    - mobilenetv4_conv_large
    - mobilenetv4_hybrid_medium
    - mobilenetv4_hybrid_large
    - ghostnet_100
    - ghostnetv2_100
    - ghostnetv2_130
    - ghostnetv2_160
    - repghostnet_050
    - repghostnet_080
    - repghostnet_100
    - repghostnet_130
    - repghostnet_200
    - lcnet_050
    - lcnet_075
    - lcnet_100
    - mnasnet_100
    - spnasnet_100
    - mixnet_s
    - mixnet_m
    - mixnet_l
    - mixnet_xl
    - tinynet_a
    - tinynet_b
    - tinynet_c
    - tinynet_d
    - tinynet_e
    - resnet18
    - resnet34

  seed : 1234
  batch_size: 16
  epochs: 500
  patience: 15
  max_checkpoints : 3
  num_workers: 4
  layers:
    # - [fc]
    # - [layer4, fc]
    # - [layer3, layer4, fc]
    # - [layer2, layer3, layer4, fc]
    # - [layer1, layer2, layer3, layer4, fc]
    # - [layer0, layer1, layer2, layer3, layer4, fc]
    - [all]

data:
  root: "/workspace/hojeon/model-fine-tuning/fine-tuning-project/dataset/StitchingNet"
  train_ratio: 0.7
  val_ratio: 0.15
  use_augmentation: false

training:
  project_name: "Hojeon-FinuTuning-Screening"
  # GPU가 여러 개이더라도 DataParallel을 활성화할지/비활성화할지 결정
  # (true로 설정 시, GPU가 2개 이상인 경우 DataParallel 적용)
  use_dataparallel: false
  checkpoint_base_dir: "/workspace/hojeon/model-fine-tuning/fine-tuning-project/checkpoint/seed-1234-aug-false"   # 체크포인트를 저장할 기본 경로
  freeze_layers: false # 모든 파라미터(requires_grad=True)

  # 아래 두 파라미터가 base_lr, head_lr
  base_lr: 1e-4
  head_lr: 1e-3

  # 스케줄러 사용 여부 및 설정 (cosine annealing)
  use_scheduler: true
  T_max: 75      # 보통 epoch 수만큼 지정
  eta_min: 1e-7  # learning rate가 내려갈 최소값