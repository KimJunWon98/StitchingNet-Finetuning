hyperparameters:
  model_name: "mobilenet_v3_large"   # 사용할 모델 이름 (예: mobilenet_v3_large, mobilenet_v2_custom_100 등)
  seed: 2025                         # 실험 재현성을 위한 시드 값
  batch_size: 32                     # 학습 배치 크기
  epochs: 30000                      # 최대 학습 epoch 수
  patience: 5                        # Early stopping patience
  max_checkpoints: 3                 # 저장할 최대 체크포인트 개수
  num_workers: 8                     # DataLoader의 worker 수
  layers:                            # (사용하지 않으면 [all]로 두세요)
    - [all]

quantization:
  qengine: "fbgemm"                  # 양자화 엔진: x86/서버는 "fbgemm", ARM/모바일은 "qnnpack" 권장

data:
  root: "./dataset/StitchingNet-ver2" # 데이터셋 루트 경로
  train_ratio: 0.7                    # 학습 데이터 비율
  val_ratio: 0.15                     # 검증 데이터 비율
  use_augmentation: 0                 # 데이터 증강 버전 (0: 없음, 1~3: 각 증강 버전)

training:
  project_name: "hojeon-branch"       # W&B 등 실험 관리용 프로젝트명
  use_dataparallel: false             # GPU가 2개 이상일 때 DataParallel 사용 여부
  checkpoint_base_dir: "./checkpoint/hojeon-branch/" # 체크포인트 저장 기본 경로
  freeze_layers: false                # True면 모든 파라미터를 freeze

  base_lr: 1e-4                       # 기본 learning rate
  head_lr: 1e-3                       # head(분류기) learning rate (사용하지 않으면 무시)

  use_scheduler: true                 # learning rate scheduler 사용 여부
  T_max: 200                          # CosineAnnealingLR의 T_max (일반적으로 epoch 수와 동일)
  eta_min: 1e-7                       # learning rate의 최소값