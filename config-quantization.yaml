hyperparameters:
  model_name: "mobilenet_v3_large"   # 사용할 모델 이름 (예: mobilenet_v3_large, mobilenet_v2_custom_100 등)
  seed: 2025                         # 실험 재현성을 위한 시드 값
  batch_size: 128                     # 학습 배치 크기
  epochs: 30000                      # 최대 학습 epoch 수
  patience: 50                        # Early stopping patience
  max_checkpoints: 3                 # 저장할 최대 체크포인트 개수
  num_workers: 8                     # DataLoader의 worker 수
  layers:                            # (사용하지 않으면 [all]로 두세요)
    - [all]

quantization:
  qengine: "qnnpack"                  # 양자화 엔진: x86/서버는 "fbgemm", ARM/모바일은 "qnnpack" 권장

data:
  dataset_name: "StitchingNet"  # 데이터셋 이름
  root: "./dataset/StitchingNet" # 데이터셋 루트 경로
  train_ratio: 0.7                    # 학습 데이터 비율
  val_ratio: 0.15                     # 검증 데이터 비율
  use_augmentation: 0                 # 데이터 증강 버전 (0: 없음, 1~3: 각 증강 버전)

training:
  training_flag: true                  # 학습 모드 여부
  project_name: "QAT"       # W&B 등 실험 관리용 프로젝트명
  use_dataparallel: false             # GPU가 2개 이상일 때 DataParallel 사용 여부
  checkpoint_base_dir: "./checkpoint/paper/dataset_test/batch128" # 체크포인트 저장 기본 경로
  freeze_layers: false                # True면 모든 파라미터를 freeze

  base_lr: 1e-4                       # 기본 learning rate
  head_lr: 1e-3                       # head(분류기) learning rate (사용하지 않으면 무시)

  use_scheduler: true                 # learning rate scheduler 사용 여부
  T_max: 200                          # CosineAnnealingLR의 T_max (일반적으로 epoch 수와 동일)
  eta_min: 1e-7                       # learning rate의 최소값

qat:
  use_qat: true                      # QAT 사용 여부
  fp32_checkpoint: "/home/kimjunwon/github-repo/StitchingNet-Finetuning/mobilenet_v2_custom_075__best.pt" # QAT 시작 시 사용할 FP32 체크포인트 경로
  best_model_metric: "val_loss_fq" # QAT에서 최적 모델을 선택할 기준 (val_loss_fq, val_acc 등)
  qat_epochs: 200                  # QAT 학습 epoch 수
  qat_use_scheduler: true          # QAT에서 learning rate scheduler 사용 여부
  qat_T_max: 50                    # QAT에서 CosineAnnealingLR의 T_max (일반적으로 qat_epochs와 동일)
  qat_eta_min: 1e-7            # QAT에서 learning rate의 최소값
  qat_lr: 1e-4                # QAT에서 기본 learning rate
  qat_head_lr: 1e-3          # QAT에서 head(분류기) learning rate (사용하지 않으면 무시)